{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"chapter1/","title":"Chapter\u202f1: Prompt Foundations &amp; OpenPages Integration","text":"<p>Goals: - Understand prompt engineering basics - Run a simple PII redaction prompt - Integrate a prompt into OpenPages via a button / UI  </p>"},{"location":"chapter1/#11-what-is-a-prompt","title":"1.1 What Is a Prompt?","text":"<p>A prompt is a textual instruction (plus context and variables) you send to a foundation model to elicit desired output. A well\u2011designed prompt includes:</p> <ul> <li>Context / system instructions  </li> <li>Clear task instructions  </li> <li>Variable placeholders (e.g. <code>{{Text}}</code>)  </li> <li>Output format constraints (e.g. JSON schema, headings)  </li> <li>Error / fallback handling  </li> </ul>"},{"location":"chapter1/#12-pii-redaction-example","title":"1.2 PII Redaction Example","text":"<p>We\u2019ll start with a worked prompt: PII Detection &amp; Redaction You can find the canonical prompt in <code>prompts/pii_redaction.json</code></p> <p>Prompt Template (excerpt):</p> <p><code>``text You are a privacy assistant.   Task:   1. Identify any Personally Identifiable Information (PII) in the input text.   2. Provide a redacted version of the text (replace PII with \u201c[REDACTED]\u201d).   3. Return output in **JSON** format with fields:      -</code>pii_identified<code>(list of strings)      -</code>redacted_text` (string)  </p> <p>Input: {{Text}}  </p> <p>Output: <code>json {   \"pii_identified\": [...],   \"redacted_text\": \"...\" }</code> ```  </p> <p>You can test this prompt in your prompt playground (e.g. via watsonx prompt client), feeding an input text.</p> <p>Sample input / output:</p> <ul> <li>Input (in <code>examples/inputs/example_text_pii.txt</code>)  </li> <li>Expected output in <code>examples/outputs/example_pii_redacted.json</code> </li> </ul>"},{"location":"chapter1/#13-lab-pii-redaction","title":"1.3 Lab: PII Redaction","text":"<p>Goal: wire up the prompt, run it, and integrate into OpenPages sandbox.</p> <ol> <li>Copy the prompt template from <code>prompts/pii_redaction.json</code> </li> <li>In your prompt client or notebook, substitute <code>{{Text}}</code> with a sample input  </li> <li>Execute and verify you get output matching the schema  </li> <li> <p>Using JSONata (or your mapping tool in OpenPages), extract the fields:</p> </li> <li> <p><code>pii_identified</code> \u2192 list  </p> </li> <li><code>redacted_text</code> \u2192 string  </li> </ol> <p>e.g. JSONata expression:     $eval(results.generated_text).pii_identified</p> <ol> <li>In the OpenPages sandbox, configure a button (or UI) that passes the record description to the prompt, receives output, and writes back results to designated fields  </li> <li>Test a few edge cases (text with no PII, multiple PII types, partial overlap)  </li> </ol>"},{"location":"chapter1/#14-tips-common-pitfalls","title":"1.4 Tips &amp; Common Pitfalls","text":"<ul> <li>Always constrain the output (e.g. \u201coutput only JSON\u201d) to avoid extra prose.  </li> <li>Use stop sequences to prevent trailing tokens.  </li> <li>For edge cases, include fallback text (\u201cIf no PII found, return <code>pii_identified: []</code>).  </li> <li>Validate the output format before writing back into systems.</li> </ul> <p>At the end you could also include links: \u201cProceed to Chapter\u202f2 \u2192 Summarization &amp; Tagging.\u201d</p>"},{"location":"chapter1/lab_pii_redaction/","title":"Lab: PII Redaction Prompt","text":""},{"location":"chapter1/lab_pii_redaction/#objective","title":"Objective","text":"<p>Implement the PII redaction prompt, run it, parse results, and integrate with OpenPages UI.</p>"},{"location":"chapter1/lab_pii_redaction/#steps","title":"Steps","text":"<ol> <li>Open the prompt template: <code>prompts/pii_redaction.json</code> </li> <li>Load the example input: <code>examples/inputs/example_text_pii.txt</code> </li> <li>Replace placeholder <code>{{Text}}</code> with the example input  </li> <li>Run the prompt in your prompt client (or notebook)  </li> <li>Inspect returned JSON, compare against <code>examples/outputs/example_pii_redacted.json</code> </li> <li>Use JSONata or mapping logic to extract <code>pii_identified</code> and <code>redacted_text</code> </li> <li>Inside OpenPages sandbox, create a UI component (e.g. a button) that:</li> <li>Takes the record\u2019s relevant text  </li> <li>Sends it to the prompt  </li> <li>Parses returned JSON  </li> <li>Updates record fields (e.g. <code>piiList</code>, <code>redactedDescription</code>)  </li> <li>Test with multiple input variations  </li> <li>Document any mis\u2011parses or format errors  </li> </ol>"},{"location":"chapter1/lab_pii_redaction/#extension-tasks","title":"Extension Tasks","text":"<ul> <li>Modify the prompt to also return the character indexes (start/end) of each PII occurrence  </li> <li>Add optional types (e.g. \u201cPhone number,\u201d \u201cEmail\u201d) and group by type  </li> <li>Add a fallback clause: if no PII detected, return an empty list (don\u2019t error)</li> </ul>"},{"location":"chapter2/","title":"Lab 1: Integrating a Custom ML Model for Risk Classification in OpenPages","text":"<p>This lab walks through how to configure and use the Custom Machine Learning Models feature in IBM OpenPages 9.0.0 to automatically classify risk descriptions into your two\u2011level taxonomy.</p> <p>You will:</p> <ol> <li>Set up and deploy the prompt in Prompt Lab</li> <li>Configure a connection to your deployed model </li> <li>Map input fields  </li> <li>Map output fields and JSONata expressions  </li> <li>Configure user guidance  </li> <li>Add the model to a view  </li> <li>Test the integration  </li> </ol> <p>Prerequisites &amp; Notes - You must have Custom Machine Learning Models permission in OpenPages, otherwise the menu is not visible. :contentReference[oaicite:0]{index=0} - You must already have deployed your classification model (e.g. on Watson ML or another AI service). :contentReference[oaicite:1]{index=1} - You must know the exact enum values for your OpenPages fields (Level\u202f1 / Level\u202f2 classification) so they match the model output. - You should be familiar with JSONata syntax to parse model outputs. :contentReference[oaicite:2]{index=2}  </p>"},{"location":"chapter2/op_integration/","title":"Lab 1.2: Integrate the model in OpenPages","text":""},{"location":"chapter2/op_integration/#1-access-the-custom-machine-learning-models-configuration","title":"1. Access the Custom Machine Learning Models Configuration","text":"<ol> <li>Log in to OpenPages as an admin (with required permissions).  </li> <li> <p>From the Administration menu, go to Integrations \u2192 Custom Machine Learning Models. </p> </li> <li> <p>Click New Model to begin configuring a new integration. </p> </li> </ol>"},{"location":"chapter2/op_integration/#2-configure-model-access-connection","title":"2. Configure Model Access / Connection","text":"<p>On the \u201cNew Model\u201d screen:</p> <ul> <li>Enter Name and Label (e.g. <code>risk_classification</code>)  </li> <li>Select the AI Service Type (e.g. Watson Machine Learning on IBM Cloud)  </li> <li>Enter Access Parameters such as:  </li> <li>Watson service type: <code>Watson Machine Learning on Cloud</code></li> <li>Authentication URL: <code>https://iam.cloud.ibm.com/identity/token</code></li> <li>API Key: Retrieve from IBM Cloud </li> <li>Base Deployment URL: <code>https://us-south.ml.cloud.ibm.com/ml/v4</code></li> <li>Deployment ID: Retrieve from deployment space on watsonx.ai</li> <li>Space ID:  Retrieve from model/prompt template deployment on watsonx.ai</li> <li>API version: <code>2021-05-01</code> </li> <li>Click Test Connection to verify connectivity </li> </ul> <p>Generating your watsonx API key for OpenPages: - Go to ibm - Via the tool bar, go to Manage &gt; Acces (IAM) </p> <ul> <li> <p>Create a new API Key that will be used for inferencing from OpenPages </p> </li> <li> <p>Copy the new API Key to enter into the \"New Model\" sceen. Make sure you keep this key a secret. </p> </li> </ul> <p>After entering valid details, click Next to proceed.</p>"},{"location":"chapter2/op_integration/#3-map-the-inputs-model-openpages-fields","title":"3. Map the Inputs (Model \u2192 OpenPages fields)","text":"<p>On the Map Inputs page:</p> <ol> <li> <p>Choose the Object Type (Risk) to which this model applies. </p> </li> <li> <p>Decide whether input is Manual or Automatic mapping.  </p> </li> <li> <p>Add one row per input your model expects:</p> </li> <li>Model input field (must match the name used by your deployed model, e.g. <code>risk_description</code>)  </li> <li>Select the corresponding OpenPages field (e.g. <code>Description</code>)  </li> <li> <p>Mark whether the input is Required </p> </li> <li> <p>Click Next to move to output mapping.  </p> <p>(If \u201cNext\u201d is disabled, check that at least one input row is defined.)</p> </li> </ol>"},{"location":"chapter2/op_integration/#4-map-the-outputs-and-jsonata-extraction","title":"4. Map the Outputs and JSONata Extraction","text":"<p>On the Map Outputs page:</p> <ol> <li> <p>Select the Insight type, e.g. <code>Set fields</code> (because you want the model to populate the Level 1 / Level 2 fields) </p> </li> <li> <p>Choose whether each output is Single insight or List of insights (for your taxonomy, Single is typical) :contentReference[oaicite:3]{index=3}  </p> </li> <li>For each output you want (e.g. Level\u202f1 classification, Level\u202f2 classification):</li> <li>Output label (e.g. <code>PrimaryClassification</code>, <code>SecondaryClassification</code>)  </li> <li> <p>JSONata expression to extract the value from the model\u2019s JSON response  </p> <ul> <li>Example: if model\u2019s JSON is: <code>json    {      \"level_1_classification\": \"Clients Products and Business Practices\",      \"level_2_classification\": \"Product Flaws\"    }</code>    then JSONata expressions might be:  </li> <li><code>level_1_classification</code> </li> <li><code>level_2_classification</code> </li> <li>If the response is wrapped (e.g. under <code>results.generated_text</code>), you may need <code>results.generated_text.level_1_classification</code> :contentReference[oaicite:4]{index=4}  </li> </ul> </li> <li> <p>(If Insight type = Set fields) Target field: map the output to your OpenPages enumerated field  </p> </li> <li> <p>Optionally set Confidence score or Minimum confidence thresholds if your scenario requires filtering suggestions. :contentReference[oaicite:5]{index=5}  </p> </li> <li>(If using <code>Set fields</code>) choose whether suggestions are User set or Automatically set. :contentReference[oaicite:6]{index=6}  </li> <li>Click Next to proceed to guidance configuration.</li> </ol>"},{"location":"chapter2/op_integration/#5-configure-user-guidance-and-display","title":"5. Configure User Guidance and Display","text":"<p>On the Guidance page:</p> <ol> <li>Enter a Description explaining what the model does (e.g. \u201cThis model classifies text into risk taxonomy\u201d)  </li> <li>Optionally set Notification Messages or Style / display options for how users see the model\u2019s suggestion. :contentReference[oaicite:7]{index=7}  </li> <li>Optional: you can embed JSONata-based conditions for alerts.  </li> <li>Click Save to complete model setup.   <p>The model should now appear in the Custom Machine Learning Models table with status \u201cComplete\u201d (or \u201cV Complete\u201d). :contentReference[oaicite:8]{index=8}  </p> </li> </ol>"},{"location":"chapter2/view_customisation/","title":"Lab 1.3: Making the AI feature available in OpenPages","text":""},{"location":"chapter2/view_customisation/#1-add-the-model-to-a-view","title":"1. Add the Model to a View","text":"<ol> <li> <p>The AI model now must be integrated into the view of where we want to interact with it. We want this model on the Risk object so we must navigate to the relevant view. Go to any risk and then turn on debug info via the Administration Menu. </p> </li> <li> <p>The view will then appear beneath the risk heading. Navigate to the view to customize it by clicking on the view name. In the view designer for your object (e.g. Risk view), find the option to add the View AI insight button and drag it into the area on the view in which you want to be able to interact with it. For this use-case, we will add it to the Risk Categorisation area.   </p> </li> <li> <p>Add the relevant details to the button parameters to connect your model  </p> </li> <li> <p>Save or Publish the view so the model is active in that view. </p> </li> </ol>"},{"location":"chapter2/view_customisation/#2-test-the-model-in-openpages","title":"2. Test the Model in OpenPages","text":"<ol> <li> <p>Navigate to a Risk record in the view where the model is active. </p> </li> <li> <p>Fill in the description (or whatever text triggers the model).  </p> </li> <li> <p>A lightbulb / insight icon should appear indicating the model can run (or automatically run). :contentReference[oaicite:9]{index=9}  </p> </li> <li> <p>Click on it and observe the side panel with model output suggestions (for Level\u202f1 and Level\u202f2). </p> </li> </ol> <p></p> <ol> <li> <p>Validate whether the classifications match expectations.  </p> </li> <li> <p>Test edge cases (no clear classification, ambiguous text) to ensure robustness.  </p> </li> </ol>"},{"location":"chapter2/view_customisation/#3-troubleshooting-tips-for-your-risk-taxonomy-use-case","title":"3. Troubleshooting &amp; Tips (for Your Risk Taxonomy Use Case)","text":"<ul> <li>Ensure the model prompt or API returns exact enum strings that match OpenPages enumerated values.  </li> <li>The JSON must be clean, valid JSON (no extra text) so JSONata evaluation succeeds.  </li> <li>If enum mismatches still occur, build a mapping / post\u2011processor between model output and OpenPages options.  </li> <li>Use the minimum confidence threshold to suppress low-confidence results.  </li> <li>Iteratively refine your prompt / model training to reduce misclassifications or \u201cno insight found\u201d cases.  </li> <li>Use Debug Info display to view raw JSON and help in refining JSONata expressions or prompt logic.</li> </ul>"},{"location":"chapter2/view_customisation/#4-example-json-prompt-response-for-your-taxonomy","title":"4. Example JSON Prompt / Response for Your Taxonomy","text":"<p>Below is a skeleton prompt and expected response format (to help you align model output with JSONata):</p> <p>Prompt (to model):</p> <p>```text You are a classifier. Given a description of a risk event, pick one Level\u202f1 (primary) and one Level\u202f2 (subcategory) from this taxonomy:</p> <p>Level\u202f1 options: - Internal Fraud - External Fraud - Employment Practices and Workplace Safety - Clients Products and Business Practices - Damage to Physical Assets - Business Disruption and System Failures - Execution Delivery and Process Management</p> <p>Level\u202f2 options (examples per Level\u202f1): - Internal Fraud \u2192 Theft and Fraud, Unauthorized Activity - External Fraud \u2192 Systems Security, Theft and Fraud - Employment Practices and Workplace Safety \u2192 Employee Relations, Safe Environment, Diversity and Discrimination - Clients Products and Business Practices \u2192 Product Flaws, Selection Sponsorship and Exposure, Improper Business or Market Practices, Suitability, Disclosures or Fiduciary, Advisory Activities - Damage to Physical Assets \u2192 Wilful damage, Disaster and other events - Business Disruption and System Failures \u2192 Infrastructure and Systems - Execution Delivery and Process Management \u2192 Vendors and Suppliers, Trade Counterparties, Transaction Data Management, Customer Client Account Management, Reporting and Disclosure, Customer Intake and Documentation  </p> <p>Return only valid JSON in this form (no extra text):</p> <p>```json {   \"level_1_classification\": \"Clients Products and Business Practices\",   \"level_2_classification\": \"Product Flaws\" }</p>"},{"location":"chapter2/wxai/","title":"Lab 1.1: Creating and Deploying a model in watsonx.ai","text":""},{"location":"chapter2/wxai/#1-set-up-and-deploy-the-prompt-in-prompt-lab","title":"1. Set Up and Deploy the prompt in Prompt Lab","text":"<ol> <li> <p>Create a new Prompt Lab asset in your project. </p> </li> <li> <p>Craft your prompt, with reference to what bits of information will be comijhg from the object you want the AI feature on. Add this as a variable in the prompt. </p> </li> <li> <p>Save the prompt as a prompt template. </p> </li> <li> <p>Promote asset to deployment space. </p> </li> <li> <p>Deploy asset in deployment space. </p> </li> <li> <p>Note down the deployment ID. We will need this in OpenPages. </p> </li> </ol>"},{"location":"chapter3/1_modify_op_object/","title":"Lab 2.1: Modifying an Object in OpenPages","text":"<p>In order to display the AI results, we need to modify the relevant OpenPages object, Control, to add fields pertaining to the results we want to see. </p>"},{"location":"chapter3/1_modify_op_object/#1-adding-object-fields-to-relevant-object-type","title":"1. Adding Object Fields to Relevant Object Type","text":"<ol> <li> <p>From the Administration menu, go to Solution Configuration \u2192 Object </p> </li> <li> <p>Find Control in the search table.</p> </li> <li> <p>Expand Fields and look for the Control AI field group. This is where we will add our 5W AI analysis output - the control quality rating.   </p> </li> <li> <p>Via the Administration menu, click on Enable System Admin Mode. This creates a database lock so that object fields can be modified.  </p> </li> <li> <p>Back in the Control object configuration, Click on New Field +.</p> </li> <li> <p>Add a meaningful name, label and description. The data type is  and The data type is <code>Enumnerated String</code> with the values matching to the taxonomy that should be output by the AI model.  </p> </li> <li> <p>Save, and disable system admin mode after the field group has been created.</p> </li> </ol>"},{"location":"chapter3/1_modify_op_object/#2-adding-the-new-object-fields-to-the-control-view","title":"2. Adding the New Object Fields to the Control View","text":"<ol> <li> <p>As in Lab 1.3, navigate to the view customiser for a Control.</p> </li> <li> <p>Add in the new field created for the 5W Control Rating into the view </p> </li> <li> <p>Publish the view.</p> </li> </ol>"},{"location":"chapter3/2_set_up_and_integrate_ai/","title":"Lab 2.2: Set-up and Integrate AI Model","text":""},{"location":"chapter3/2_set_up_and_integrate_ai/#1-create-and-deploy-the-prompt-template-in-watsonxai","title":"1. Create and Deploy the Prompt Template in watsonx.ai","text":"<ol> <li>As per Lab 1.1, create an AI deployment in watsonx.ai for 5W analysis. Use the Control Description as input, and output each of the 5 Ws as well as the Control Rating. Ensure that the Control Rating output aligns with the taxonomy created in Lab 2.1. Here is something to get you started:</li> </ol> <pre><code>[INSTRUCTION]\n\n&gt; Define the purpose of the AI and it's task &lt;\n\nControl Description Format:\n&gt; Define the input and the definition of each 5W it should ideally contain &lt;\n\nOutput you need to give within a single json object:\n1. &gt; Provide the instruction to extract the 5Ws &lt;\n2. &gt; Provide the instruction to generate the Control Quality &lt; \n4. &gt; Provide the instruction to generate the explanation of the Control Quality &lt;\n\nGenerate only a json object, no post script tagging such as [END OF INSTRUCTION].\n\n[FEW-SHOT EXAMPLES]\n\nExample 1:\nControl Description:\n\"All automated and manual alerts must be assessed in a timely manner in line with the Investigations Standard Operating Procedure (SOP) by investigators and adhere to internal tolerances (WHY). The requirements are outlined in the Institutional Investigations SOP and the Investigation Timing, Tolerances &amp; Appetite document. The Investigations team Lead (WHO) monitors the timeliness of alert assessment at least twice weekly (WHEN) via the Transaction Monitoring and Investigations Dashboard. Metrics (WHAT) are discussed at the monthly Finance Oversight Forum prior to being presented at the Global Working Group Forum. Exceptions are escalated to Head of Finance. Email evidence of escalations and metrics reported to the forums are saved in the Finance SharePoint (WHERE).\"\n\nQuestion:\nWhat is the JSON Output with no markup and no backticks for 5W description analysis?\n\nResponse (JSON Output):\n{\n\"Who\": &gt; The who &lt;,\n\"What\": &gt; The what &lt;,\n\"When\": &gt; The when &lt;,\n\"Where\": &gt; The where &lt;,\n\"Why\": &gt; The why &lt;,\n\"Control Quality\": &gt; The control quality &lt;,\n\"Control Quality Explain\": &gt; An explanation of why the control quality is the way it is&lt;\n}\n\nExample 2:\nControl Description:\n\"Multiple levels of review and challenge of the stress testing assumptions and results, from R&amp;CA team members to Division Head to CEO of the Group and the Bank.\"\n\nQuestion:\nWhat is the JSON Output with no markup and no backticks for 5W description analysis?\n\nResponse (JSON Output):\n{\n\"Who\": &gt; The who &lt;,\n\"What\": &gt; The what &lt;,\n\"When\": &gt; The when &lt;,\n\"Where\": &gt; The where &lt;,\n\"Why\": &gt; The why &lt;,\n\"Control Quality\": &gt; The control quality &lt;,\n\"Control Quality Explain\": &gt; An explanation of why the control quality is the way it is&lt;\n}\n\nExample 3:\nControl Description:\n\"Why: The control objective is to ensure the adequacy of professional indemnity insurance arrangements for Customer Finance.\\nWhat: On an annual basis, Custody and Product Management review the adequacy of the compensation and insurance arrangements for Customer Finance. In assessing the adequacy of the PI insurance cover, the following information is reviewed: \\n\u2022\\tCustomer Finance\u2019s business activities and financial position \\n\u2022\\tInformation about the PI insurance cover against the requirements under ASIC Regulatory Guide\nRG126\\nWho: The review is coordinated by The Office of the Trustee with the input from Product Management. \\n\" \n\nQuestion:\nWhat is the JSON Output with no markup and no backticks for 5W description analysis?\n\nResponse (JSON Output):\n{\n\"Who\": &gt; The who &lt;,\n\"What\": &gt; The what &lt;,\n\"When\": &gt; The when &lt;,\n\"Where\": &gt; The where &lt;,\n\"Why\": &gt; The why &lt;,\n\"Control Quality\": &gt; The control quality &lt;,\n\"Control Quality Explain\": &gt; An explanation of why the control quality is the way it is&lt;\n}\n\nExample 4:\nControl Description:\n\"[WHO] Prudential Risk CARM, \\n[WHAT] as data consumers, provides performs appropriate checks and reviews of input data, including through variance analysis and four-eyes checks (multiple reviews within Prudential Risk CARM) as outlined in the calculation, Monitoring and Reporting procedures \\n[WHEN] Every quarter, both before (input data check) and after (variance analysis, four-eyes check) generating dashboard, \\n[WHY] To verify the input data and how it is processed by the dashboard, and identify data issues which may require rectification  \\n[WHERE] Evidence of the above review and variance analysis is saved in the SharePoint.\"\n\nQuestion:\nWhat is the JSON Output with no markup and no backticks for 5W description analysis?\n\nResponse (JSON Output):\n{\n\"Who\": &gt; The who &lt;,\n\"What\": &gt; The what &lt;,\n\"When\": &gt; The when &lt;,\n\"Where\": &gt; The where &lt;,\n\"Why\": &gt; The why &lt;,\n\"Control Quality\": &gt; The control quality &lt;,\n\"Control Quality Explain\": &gt; An explanation of why the control quality is the way it is&lt;\n}\n\nExample 5:\nControl Description:\n\"[WHO] Credit Group [WHAT] notify senior management and escalate [WHEN] which have exceeded APS221 prudential limit thresholds of 25%. [WHY] This is to support regulatory reporting obligations and to provide transparency over management and mitigation of material large exposures.[WHERE] Evidence of large exposure escalations are recorded in an appropriate register.\"\n\nQuestion:\nWhat is the JSON Output with no markup and no backticks for 5W description analysis?\n\nResponse (JSON Output):\n{\n\"Who\": &gt; The who &lt;,\n\"What\": &gt; The what &lt;,\n\"When\": &gt; The when &lt;,\n\"Where\": &gt; The where &lt;,\n\"Why\": &gt; The why &lt;,\n\"Control Quality\": &gt; The control quality &lt;,\n\"Control Quality Explain\": &gt; An explanation of why the control quality is the way it is&lt;\n}\n\n[INPUT]\n\nControl Description:\n{control_description}\n\nQuestion:\nWhat is the JSON Output with no markup and no backticks for 5W description analysis?\n\nResponse (JSON Output):\n</code></pre>"},{"location":"chapter3/2_set_up_and_integrate_ai/#2-integrating-deployed-ai-model-to-openpages","title":"2. Integrating Deployed AI Model to OpenPages","text":"<ol> <li> <p>Once the AI model is deployed in watsonx.ai, it can be integrated into OpenPages. As per Lab 1.2, configure a Custom Machine Learning Model and set up the Model access and Inputs.  </p> </li> <li> <p>For Outputs, add outputs for headers for each of the 5Ws (for formatting), as well as the actual output text with the JSONata string that translates the AI model output. Ensure that the Control Rating is set to the target field created in lab 2.1.   </p> </li> <li> <p>Add a suitable dscription and customise the style to your choosing in Guidance. Save the model.</p> </li> </ol>"},{"location":"chapter3/2_set_up_and_integrate_ai/#3-add-the-model-to-control-view","title":"3. Add the Model to Control View","text":"<ol> <li> <p>Navigate to the control object view as per Lab 2.1 to add the AI model we deployed.</p> </li> <li> <p>Rather than add a new AI button, navigate to the Description field and add the 5W model to AI model configuration. Click Done and publish the view. </p> </li> </ol>"},{"location":"chapter3/2_set_up_and_integrate_ai/#4-test-the-model-in-openpages","title":"4. Test the Model in OpenPages","text":"<ol> <li>Find an example control, and run the 5W AI model that appears near the control description. The rating output will populate the 5W Control Rating field created before. </li> </ol>"},{"location":"chapter3/3_bulk_analysis/","title":"Lab 2.3: Execute AI in Bulk","text":"<p>So far, we have been executing AI through user interactions - let's look at how we can apply AI at scale. </p>"},{"location":"chapter3/3_bulk_analysis/#1-create-workflow","title":"1. Create workflow","text":"<ol> <li> <p>So that we don't execute the AI against every single control, first identify candidate records. We can do this by adding a dummy field called '5W Candidate', adding it to the Control view and then setting this as a valid value for a few controls. Later we will add it as a condition for the workflow.</p> </li> <li> <p>From the Administration menu, go to Solution Configuration \u2192 Workflows </p> </li> <li> <p>Create a new workflow. Provide the following parameters:</p> </li> <li>Name: Add a suitable name</li> <li>Object type: <code>Control</code></li> <li>Choose how the workflow starts: <code>User action or scheduler</code> </li> <li>Allow user to start workflow through the task view: <code>Yes</code></li> <li>Enabled: <code>Yes</code></li> <li> <p>Automated: <code>No</code> </p> </li> <li> <p>Add only a start and end block. </p> </li> <li> <p>Add an action connecting the start to the end block. Provide the following:</p> </li> <li>Name: Add a suitable name</li> <li>Label: Add a suitable label</li> <li>Run In Background: <code>False</code> </li> <li> <p>Auto-Advance Stage: <code>True</code> </p> </li> <li> <p>Provide no conditions. Click New Operation + under Validations and Operations to add the AI model. </p> </li> <li> <p>For the new operation, provide the following:</p> </li> <li>Operation: <code>Run an AI Model</code></li> <li>Name: Add a suitable name</li> <li>When: Keep empty</li> <li>Advanced Logic: <code>False</code> </li> <li>Execute As System: <code>False</code> </li> <li>Target Objects: <code>Self</code> </li> <li> <p>Model: Select your deployed AI model </p> </li> <li> <p>Clicking on the empty area in the canvas will display the Workflow Properties. Scroll down to Applicability and click New Condition +.</p> </li> <li> <p>In the new condition, ensure that the 5W Candidate field created earlier is equal to a specific value that you set candidate records as. </p> </li> <li> <p>Publish the workflow.</p> </li> </ol>"},{"location":"chapter3/3_bulk_analysis/#2-view-results-at-scale","title":"2. View Results at Scale","text":"<ol> <li> <p>Modify the Control Grid View to see the 5W results at a high level. Via the hamburger menu, navigate to Assessments &gt; Controls.</p> </li> <li> <p>With Debug Info displayed, naviagte to the View for the Control Grid.  [screenshot with cursor hovering over the control grid view name] </p> </li> <li> <p>Add the 5W Control Rating field to the grid and publish the view. </p> </li> </ol>"},{"location":"chapter3/3_bulk_analysis/#3-test-the-bulk-ai-execution","title":"3. Test the Bulk AI Execution","text":"<ol> <li> <p>Go back to the workflows page.</p> </li> <li> <p>Select the workflow created for the 5W Bulk Control Analysis and start the workflow. </p> </li> <li> <p>We can check the progress of the workflow in Administration menu, go to Other \u2192 Background Processes </p> </li> <li> <p>We can see the output by going to our Control grid view and using the 5W Candidate field as a filter </p> </li> </ol>"}]}